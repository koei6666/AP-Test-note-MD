
[[データ構造]]
[[探索アルゴリズム]]

[[構造　配列]]
[[構造　連結リスト]]
[[構造　スタック]]
[[構造　キュー]]

[[構造　木]]
[[構造　２分木]]
[[構造　バランス木]]
[[構造　B木]]
[[木の走査　DFS BFS]]
[[２分木の走査]]

[[探索　線型探索]]
[[探索　2分法]]
[[探索　ハッシュ法]]

[[整列アルゴリズム]]

[[プログラム特性]]
[[プログラム制御]]
　[[変数の記憶期間]]
　[[プログラムメモリの割り当て]]
　[[プログラムの呼び出し]]
[[プログラミング言語]]
　[[手続型言語]]
　[[関数型言語]]
　[[論理型言語]]
　[[オブジェクト指向言語]] 
　　[[Javaの特徴]]
　[[マークアップ言語]]
　[[スクリプト言語]]
[[JAVA,XMLに関連する用語]]
　



例題3
　ある #B木 は、各節点に４個のキーを格納し、５本の枝を出す。このB木の根（深さレベル０）から深さレベル２までの節点に格納できるキーの個数は、最大いくつか？
　解説：
　　この問題を解くには、B木の節点構造を理解する必要がある。
　　B木の節は下のような構造となっている：
　　|p0|k1|p1|k2|p2|......|kn|pn|
　　一つの節にはキーをn個まで格納できる（n：偶数）
　　p0〜pnには子へのポインタが格納される
　　k1〜knは、節内で昇順で並んでいる。
　　[[構造　B木]]
　　それでは問題には、各節に５本の枝を出していることから、５個のポインタを格納していることがわかるので、以下の計算ができる： ^5aae82

	　　level0: root=1node -> point to 5 children
	　　level1: 5 nodes -> point to 5 children each = 25 children in toal
	　　level2: 25 nodes

　　　そして、問題文からは各節点には４個のキーがあるとのことで、以下計算で答えを導くことができる：

	　　　25+5+1=31
	　　　31*4=124
　

例題4　n個のデータを整列するとき、比較回数が最悪の場合でO(n^2)、最良の場合でO(n)となるものはどれか。
　　解説：
here's a comparison matrix for the computational complexity of several common sorting algorithms. This includes both time complexity and space complexity, both in the average and worst-case scenarios. ^bfa637

| Sorting Algorithm  | Best Case Time Complexity | Average Case Time Complexity | Worst Case Time Complexity | Space Complexity |
|-------------------|---------------------------|------------------------------|----------------------------|------------------|
| Bubble Sort       | O(n)                      | O(n^2)                       | O(n^2)                     | O(1)             |
| Selection Sort    | O(n^2)                    | O(n^2)                       | O(n^2)                     | O(1)             |
| Insertion Sort    | O(n)                      | O(n^2)                       | O(n^2)                     | O(1)             |
| Heap Sort         | O(n log n)                | O(n log n)                   | O(n log n)                 | O(1)             |
| Quick Sort        | O(n log n)                | O(n log n)                   | O(n^2)                     | O(log n)         |
| Merge Sort        | O(n log n)                | O(n log n)                   | O(n log n)                 | O(n)             |
| Radix Sort        | O(nk)                     | O(nk)                         | O(nk)                       | O(n+k)           |
| Bucket Sort       | O(n+k)                    | O(n+k)                       | O(n^2)                     | O(nk)            |
| Shell Sort        | O(n)                      | O(n(log n)^2)                | O(n(log n)^2)              | O(1)             |
| Tim Sort          | O(n)                      | O(n log n)                   | O(n log n)                 | O(n)             |

Here's a quick guide on how to calculate computational complexity:

1. **Identify Input Size**: The first step is identifying the size of the input to your algorithm. For sorting algorithms, this is typically the number of elements you're trying to sort (n).

2. **Identify Basic Operations**: Basic operations are the simple computational steps that your algorithm performs - these could be arithmetic operations, comparisons, or data movements.

3. **Count Operations**: Count how many of these basic operations your algorithm will perform in the worst case. This usually involves constructing a function that relates the size of the input to the number of operations. 

4. **Find Asymptotic Upper Bound**: Asymptotic complexity (Big O notation) describes how the run time scales as the input size grows. The most common way to express time complexity is using Big O notation. This encapsulates the upper bound of the time requirement in the worst-case scenario.

Please note that time complexity doesn't count the actual time required, but the number of operations. As such, it describes the rate of increase of time taken with respect to input size.

Here are some examples:

- Linear Time, O(n): If doubling the size of the input doubles the amount of work, the algorithm is said to have linear time complexity.
  
- Logarithmic Time, O(log n): If doubling the size of the input doesn't double the work, but instead increases it by a constant amount, the algorithm is said to have logarithmic time complexity.
  
- Quadratic Time, O(n^2): If doubling the size of the input quadruples the work, the algorithm is said to have quadratic time complexity.
  
Understanding time complexity and how to calculate it is a crucial aspect of designing efficient algorithms.

==同じく計算量を計算する過去問==
[[過去問#^5606d7]]